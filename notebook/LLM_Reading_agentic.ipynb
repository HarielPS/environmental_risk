{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25b9f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4050 Laptop GPU, compute capability 8.9, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión llama-cpp-python: 0.3.16\n",
      "GPU offload soportado: True\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "from llama_cpp import llama_supports_gpu_offload\n",
    "\n",
    "print(\"Versión llama-cpp-python:\", llama_cpp.__version__)\n",
    "print(\"GPU offload soportado:\", llama_supports_gpu_offload())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ff7828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import fitz  # pymupdf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbc130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH: /home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/models/qwen2.5-3b-instruct-q5_k_m.gguf\n",
      "INPUT_JSON_PATH: /home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/input_LLM.json\n",
      "RAG_NORMAS_DIR: /home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/rag_normas\n",
      "OUTPUT_JSON_PATH: /home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/output_LLM.json\n"
     ]
    }
   ],
   "source": [
    "# ===== RUTAS EN TU MONOREPO =====\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "MODEL_PATH = REPO_ROOT / \"models\" / \"qwen2.5-3b-instruct-q5_k_m.gguf\"\n",
    "\n",
    "INPUT_JSON_PATH = REPO_ROOT / \"Data\" / \"input_LLM.json\"\n",
    "\n",
    "OUTPUT_JSON_PATH = REPO_ROOT / \"Data\" / \"output_LLM.json\"\n",
    "\n",
    "RAG_NORMAS_DIR = REPO_ROOT / \"Data\" / \"rag_normas\"\n",
    "\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"INPUT_JSON_PATH:\", INPUT_JSON_PATH)\n",
    "print(\"RAG_NORMAS_DIR:\", RAG_NORMAS_DIR)\n",
    "print(\"OUTPUT_JSON_PATH:\", OUTPUT_JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2cb304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (6000) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded OK\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=str(MODEL_PATH),\n",
    "    n_ctx=6000,\n",
    "    n_threads=os.cpu_count() or 8,\n",
    "    n_gpu_layers=10,          # si tienes CUDA: prueba 20, 30, 35...\n",
    "    chat_format=\"chatml\",    # Qwen2.5 Instruct suele ir bien con ChatML\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"LLM loaded OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1c8b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obra_id', 'obra_tipo', 'obra_descripcion', 'ubicacion', 'normativa_objetivo', 'materiales', 'resumen'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(INPUT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    payload = json.load(f)\n",
    "\n",
    "payload.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e93ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normas objetivo: ['RENE', 'ISO_14064-1', 'ISO_14067', 'GHG_Protocol_Scope3', 'NACDMX-007-RNAT-2019', 'NADF-018-AMBT-2009', 'Ley_Residuos_Solidos_CDMX', 'Reglamento_Construcciones_CDMX', 'NOM-001-SEDE-2012']\n",
      "Páginas seleccionadas (docs): 5\n",
      "Docs faltantes: 4\n",
      "RAG pages (total): 194\n",
      "Manifest guardado en: /home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/rag_normas/page_manifest.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc_key': 'ISO_14064-1',\n",
       "  'path': '/home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/rag_normas/ISO_14064-1_2018.pdf',\n",
       "  'selected_pages_0index': [1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57],\n",
       "  'extraction_mode_stats': {'text_layer': 39, 'ocr': 0}},\n",
       " {'doc_key': 'ISO_14067',\n",
       "  'path': '/home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/rag_normas/UNE-EN-ISO-14067.pdf',\n",
       "  'selected_pages_0index': [0, 1, 2, 3, 4],\n",
       "  'extraction_mode_stats': {'text_layer': 5, 'ocr': 0}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RAG: Selección de páginas + OCR sólo donde haga falta\n",
    "# (para NO procesar PDFs completos como NOM-001 (780 pág.))\n",
    "# ============================================================\n",
    "\n",
    "# --- (A) Resolver PDFs (busca primero en Data/rag_normas, si no, usa rutas locales) ---\n",
    "# Ajusta estos nombres para que coincidan con tus archivos reales.\n",
    "PDF_SOURCES = {\n",
    "    \"NOM-001-SEDE-2012\": [\"NOM-001-SEDE-2012.pdf\"],\n",
    "    \"Reglamento_Construcciones_CDMX\": [\"RGTO_CONSTRUCCIONES_22_04_2022.pdf\", \"Reglamento_Construcciones_CDMX.pdf\"],\n",
    "    \"NACDMX-007-RNAT-2019\": [\"GOCDMX_21-07-20_SEDEMA.pdf\", \"NACDMX-007-RNAT-2019.pdf\"],\n",
    "    \"Ley_Residuos_Solidos_CDMX\": [\"5e9cfdc1fa63fdf6120fd92f434a3e407d58af30.pdf\", \"Ley_Residuos_Solidos_CDMX.pdf\"],\n",
    "    \"GHG_Protocol_Scope3\": [\"protocolo_spanish.pdf\", \"GHG_Protocol.pdf\"],\n",
    "    \"ISO_14064-1\": [\"ISO_14064-1_2018.pdf\", \"Plantilla_NORMAISO14064-1.pdf\"],\n",
    "    \"ISO_14067\": [\"UNE-EN-ISO-14067.pdf\"],\n",
    "    # Si tienes RENE en PDF, agrégalo aquí:\n",
    "    \"RENE\": [\"RENE.pdf\"],\n",
    "    # Si tienes NADF-018 en PDF, agrégalo aquí:\n",
    "    \"NADF-018-AMBT-2009\": [\"NADF-018-AMBT-2009.pdf\"],\n",
    "}\n",
    "\n",
    "def resolve_pdf_path(doc_key: str, rag_dir: Path) -> Path | None:\n",
    "    \"\"\"\n",
    "    Busca un PDF por nombre dentro de Data/rag_normas.\n",
    "    Si no existe, intenta también en /mnt/data (útil en esta demo).\n",
    "    \"\"\"\n",
    "    candidates = PDF_SOURCES.get(doc_key, [])\n",
    "    for name in candidates:\n",
    "        p1 = rag_dir / name\n",
    "        if p1.exists():\n",
    "            return p1\n",
    "        p2 = Path(\"/mnt/data\") / name\n",
    "        if p2.exists():\n",
    "            return p2\n",
    "    return None\n",
    "\n",
    "# --- (B) Keywords por norma (y también tomaremos materiales/ubicación del JSON) ---\n",
    "NORM_KEYWORDS: Dict[str, List[str]] = {\n",
    "    \"NOM-001-SEDE-2012\": [\n",
    "        \"instalaciones eléctricas\", \"vivienda\", \"Artículo 110\", \"Artículo 210\", \"Artículo 220\",\n",
    "        \"Artículo 230\", \"Artículo 240\", \"Artículo 250\", \"puesta a tierra\", \"circuitos derivados\",\n",
    "        \"alimentadores\", \"tableros\", \"contactos\", \"conductores\", \"sobrecorriente\",\n",
    "    ],\n",
    "    \"Reglamento_Construcciones_CDMX\": [\n",
    "        \"licencia\", \"manifestación de construcción\", \"obra\", \"seguridad estructural\", \"dictamen\",\n",
    "        \"director responsable de obra\", \"corresponsable\", \"protección civil\", \"demolición\",\n",
    "        \"modificación\", \"ampliación\", \"reparación\", \"instalaciones\",\n",
    "    ],\n",
    "    \"NACDMX-007-RNAT-2019\": [\n",
    "        \"residuos de la construcción\", \"demolición\", \"clasificación\", \"manejo integral\",\n",
    "        \"plan de manejo\", \"separación\", \"acopio\", \"centros de acopio\", \"bitácora\",\n",
    "        \"transporte\", \"disposición final\", \"reciclaje\", \"reutilización\",\n",
    "    ],\n",
    "    \"Ley_Residuos_Solidos_CDMX\": [\n",
    "        \"residuos\", \"gestión integral\", \"aprovechamiento\", \"valorización\", \"acopio\",\n",
    "        \"recolección\", \"transporte\", \"tratamiento\", \"disposición final\", \"sanciones\",\n",
    "        \"residuos de la construcción\", \"demolición\",\n",
    "    ],\n",
    "    \"GHG_Protocol_Scope3\": [\n",
    "        \"límite organizacional\", \"límite operacional\", \"alcance 1\", \"alcance 2\", \"alcance 3\",\n",
    "        \"emisiones indirectas\", \"cálculo\", \"factores de emisión\", \"calidad de datos\",\n",
    "        \"verificación\", \"reporte\", \"principios\",\n",
    "    ],\n",
    "    \"ISO_14064-1\": [\n",
    "        \"límites de la organización\", \"límites de informe\", \"emisiones directas\", \"emisiones indirectas\",\n",
    "        \"inventario\", \"año base\", \"cuantificación\", \"selección\", \"recopilación de datos\",\n",
    "        \"principios\", \"exactitud\", \"transparencia\",\n",
    "    ],\n",
    "    \"ISO_14067\": [\n",
    "        \"huella de carbono de productos\", \"unidad funcional\", \"unidad declarada\", \"límite del sistema\",\n",
    "        \"ciclo de vida\", \"calidad de datos\", \"objetivo y alcance\", \"metodología\", \"cuantificación\",\n",
    "    ],\n",
    "    \"RENE\": [\"energía\", \"eficiencia\", \"edificación\", \"requisitos\", \"envolvente\", \"instalaciones\"],\n",
    "    \"NADF-018-AMBT-2009\": [\"polvo\", \"partículas\", \"obra\", \"construcción\", \"control\", \"mitigación\", \"emisiones\"],\n",
    "}\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip().lower()\n",
    "\n",
    "def score_page(text: str, keywords: List[str]) -> int:\n",
    "    t = normalize_text(text)\n",
    "    score = 0\n",
    "    for kw in keywords:\n",
    "        kw_n = normalize_text(kw)\n",
    "        if not kw_n:\n",
    "            continue\n",
    "        score += t.count(kw_n)\n",
    "    return score\n",
    "\n",
    "def select_pages_by_keywords(pdf_path: Path, keywords: List[str], max_pages: int = 25, add_neighbors: int = 1) -> List[int]:\n",
    "    \"\"\"\n",
    "    Selecciona páginas con más ocurrencias de keywords (usando texto embebido del PDF).\n",
    "    NO usa OCR en esta fase (rápido). Devuelve números de página 0-index.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    scored: List[Tuple[int, int]] = []\n",
    "    for i in range(doc.page_count):\n",
    "        txt = doc.load_page(i).get_text(\"text\") or \"\"\n",
    "        s = score_page(txt, keywords)\n",
    "        if s > 0:\n",
    "            scored.append((i, s))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_pages = [p for p, _ in scored[:max_pages]]\n",
    "\n",
    "    # Expandir con páginas vecinas para no cortar contexto\n",
    "    expanded = set()\n",
    "    for p in top_pages:\n",
    "        for j in range(p - add_neighbors, p + add_neighbors + 1):\n",
    "            if 0 <= j < doc.page_count:\n",
    "                expanded.add(j)\n",
    "\n",
    "    return sorted(expanded)\n",
    "\n",
    "def try_ocr_page(pdf_path: Path, page_no: int, dpi: int = 220) -> str:\n",
    "    \"\"\"\n",
    "    OCR de una sola página. Si pytesseract no está disponible, devuelve texto vacío.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pytesseract\n",
    "        from PIL import Image\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    page = doc.load_page(page_no)\n",
    "    pix = page.get_pixmap(dpi=dpi, alpha=False)\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    text = pytesseract.image_to_string(img, lang=\"spa\")\n",
    "    return text\n",
    "\n",
    "def extract_selected_pages(pdf_path: Path, pages: List[int], ocr_if_text_lt: int = 80) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extrae contenido por página. Usa:\n",
    "    - texto embebido si hay suficiente\n",
    "    - OCR sólo si el texto embebido es muy poco (típico de páginas escaneadas)\n",
    "    \"\"\"\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    out: List[Dict[str, Any]] = []\n",
    "\n",
    "    for p in pages:\n",
    "        page = doc.load_page(p)\n",
    "        txt = (page.get_text(\"text\") or \"\").strip()\n",
    "        used = \"text_layer\"\n",
    "\n",
    "        if len(txt) < ocr_if_text_lt:\n",
    "            ocr_txt = (try_ocr_page(pdf_path, p) or \"\").strip()\n",
    "            if len(ocr_txt) > len(txt):\n",
    "                txt = ocr_txt\n",
    "                used = \"ocr\"\n",
    "\n",
    "        # limpiar mínimo\n",
    "        txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
    "        txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
    "\n",
    "        out.append({\n",
    "            \"page\": p,               # 0-index\n",
    "            \"used\": used,            # \"text_layer\" | \"ocr\"\n",
    "            \"text\": txt,\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def infer_keywords_from_payload(payload: Dict[str, Any]) -> List[str]:\n",
    "    kws: List[str] = []\n",
    "\n",
    "    # ubicación\n",
    "    ub = payload.get(\"ubicacion\", {}) or {}\n",
    "    for k in [\"pais\", \"estado\", \"alcaldia_municipio\"]:\n",
    "        if ub.get(k):\n",
    "            kws.append(str(ub[k]))\n",
    "\n",
    "    # obra tipo / descripción\n",
    "    if payload.get(\"obra_tipo\"):\n",
    "        kws.append(str(payload[\"obra_tipo\"]))\n",
    "    if payload.get(\"obra_descripcion\"):\n",
    "        kws.append(str(payload[\"obra_descripcion\"]))\n",
    "\n",
    "    # materiales\n",
    "    for m in payload.get(\"materiales\", []) or []:\n",
    "        if m.get(\"material_nombre\"):\n",
    "            kws.append(str(m[\"material_nombre\"]))\n",
    "        if m.get(\"categoria\"):\n",
    "            kws.append(str(m[\"categoria\"]))\n",
    "\n",
    "    # algunas palabras generales útiles\n",
    "    kws += [\"obra\", \"construcción\", \"vivienda\", \"residuos\", \"emisiones\", \"CO2\", \"huella de carbono\"]\n",
    "\n",
    "    # normalizar y deduplicar manteniendo orden\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for k in kws:\n",
    "        k2 = normalize_text(k)\n",
    "        if k2 and k2 not in seen:\n",
    "            seen.add(k2)\n",
    "            out.append(k)\n",
    "    return out\n",
    "\n",
    "def flatten_normas(normativa_objetivo: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convierte el objeto normativa_objetivo en una lista plana de claves.\n",
    "    \"\"\"\n",
    "    norms: List[str] = []\n",
    "    for _, arr in (normativa_objetivo or {}).items():\n",
    "        if isinstance(arr, list):\n",
    "            norms.extend(arr)\n",
    "    # dedupe\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for n in norms:\n",
    "        if n not in seen:\n",
    "            seen.add(n)\n",
    "            out.append(n)\n",
    "    return out\n",
    "\n",
    "# --- (C) Construir manifest de páginas por norma, basado en el JSON de entrada ---\n",
    "normas_objetivo = flatten_normas(payload.get(\"normativa_objetivo\", {}))\n",
    "payload_kws = infer_keywords_from_payload(payload)\n",
    "\n",
    "page_manifest: Dict[str, Any] = {\n",
    "    \"obra_id\": payload.get(\"obra_id\"),\n",
    "    \"docs\": [],\n",
    "    \"missing_docs\": [],\n",
    "}\n",
    "\n",
    "rag_pages: List[Dict[str, Any]] = []\n",
    "\n",
    "for norma in normas_objetivo:\n",
    "    # mapear norma -> doc_key del catálogo\n",
    "    doc_key = norma  # por defecto: ya coincide con tus claves (ISO_14064-1, etc.)\n",
    "\n",
    "    pdf_path = resolve_pdf_path(doc_key, RAG_NORMAS_DIR)\n",
    "    if not pdf_path:\n",
    "        page_manifest[\"missing_docs\"].append({\"doc_key\": doc_key, \"norma\": norma})\n",
    "        continue\n",
    "\n",
    "    # keywords = (específicas de la norma) + (del payload)\n",
    "    kw = list(dict.fromkeys((NORM_KEYWORDS.get(doc_key, []) + payload_kws)))\n",
    "\n",
    "    # seleccionar páginas\n",
    "    pages = select_pages_by_keywords(pdf_path, kw, max_pages=25, add_neighbors=1)\n",
    "\n",
    "    # si no encontró nada, al menos toma el índice/intro (primeras 3-5 páginas)\n",
    "    if not pages:\n",
    "        pages = list(range(0, min(5, fitz.open(str(pdf_path)).page_count)))\n",
    "\n",
    "    # extraer (OCR sólo si hace falta)\n",
    "    extracted = extract_selected_pages(pdf_path, pages, ocr_if_text_lt=80)\n",
    "\n",
    "    page_manifest[\"docs\"].append({\n",
    "        \"doc_key\": doc_key,\n",
    "        \"path\": str(pdf_path),\n",
    "        \"selected_pages_0index\": pages,\n",
    "        \"extraction_mode_stats\": {\n",
    "            \"text_layer\": sum(1 for e in extracted if e[\"used\"] == \"text_layer\"),\n",
    "            \"ocr\": sum(1 for e in extracted if e[\"used\"] == \"ocr\"),\n",
    "        },\n",
    "    })\n",
    "\n",
    "    # guardar páginas como docs RAG a nivel página (para chunking + trazabilidad)\n",
    "    for e in extracted:\n",
    "        rag_pages.append({\n",
    "            \"doc_id\": doc_key,\n",
    "            \"path\": str(pdf_path),\n",
    "            \"page\": e[\"page\"],          # 0-index\n",
    "            \"used\": e[\"used\"],\n",
    "            \"content\": e[\"text\"],\n",
    "        })\n",
    "\n",
    "# Persistir manifest (para depurar y repetir el mismo corte)\n",
    "RAG_NORMAS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "manifest_path = RAG_NORMAS_DIR / \"page_manifest.json\"\n",
    "manifest_path.write_text(json.dumps(page_manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Normas objetivo:\", normas_objetivo)\n",
    "print(\"Páginas seleccionadas (docs):\", len(page_manifest[\"docs\"]))\n",
    "print(\"Docs faltantes:\", len(page_manifest[\"missing_docs\"]))\n",
    "print(\"RAG pages (total):\", len(rag_pages))\n",
    "print(\"Manifest guardado en:\", manifest_path)\n",
    "page_manifest[\"docs\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7aabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1283.04it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings ready: (995, 384)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Chunking (desde páginas seleccionadas) + Embeddings\n",
    "# Cada chunk conserva: doc_id + page + chunk_id\n",
    "# ============================================================\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 900, overlap: int = 120) -> List[str]:\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if end == len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "all_chunks: List[Dict[str, Any]] = []\n",
    "for d in rag_pages:\n",
    "    for i, ch in enumerate(chunk_text(d[\"content\"], chunk_size=900, overlap=140)):\n",
    "        all_chunks.append({\n",
    "            \"doc_id\": d[\"doc_id\"],\n",
    "            \"page\": d[\"page\"],  # 0-index\n",
    "            \"used\": d[\"used\"],\n",
    "            \"chunk_id\": f\"{d['doc_id']}::p{d['page']:04d}::chunk{i:04d}\",\n",
    "            \"text\": ch,\n",
    "        })\n",
    "\n",
    "print(\"Total chunks:\", len(all_chunks))\n",
    "\n",
    "# Embeddings\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "chunk_texts = [c[\"text\"] for c in all_chunks]\n",
    "chunk_embs = embed_model.encode(chunk_texts, normalize_embeddings=True)\n",
    "\n",
    "print(\"Embeddings ready:\", chunk_embs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d6068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        doc_id  page        used                       chunk_id  \\\n",
      "0  ISO_14064-1     1  text_layer  ISO_14064-1::p0001::chunk0000   \n",
      "1  ISO_14064-1     1  text_layer  ISO_14064-1::p0001::chunk0001   \n",
      "2  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0000   \n",
      "3  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0001   \n",
      "4  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0002   \n",
      "5  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0003   \n",
      "6  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0004   \n",
      "7  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0005   \n",
      "8  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0006   \n",
      "9  ISO_14064-1     2  text_layer  ISO_14064-1::p0002::chunk0007   \n",
      "\n",
      "                                                text  \n",
      "0  ﻿ ISO 14064-1:2018 (traducción oficial) ﻿ DOCU...  \n",
      "1  iza Versión española publicada en 2019 Traducc...  \n",
      "2  ﻿ ISO 14064-1:2018 (traducción oficial) ﻿ Pról...  \n",
      "3  .................................................  \n",
      "4  ............................................5 ...  \n",
      "5  ......................................... 7 4....  \n",
      "6  .................................................  \n",
      "7  orme.............................................  \n",
      "8  fuentes y sumideros de GEI.......................  \n",
      "9  e GEI............................................  \n"
     ]
    }
   ],
   "source": [
    "# (opcional) tabla rápida para inspección\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(all_chunks[:10])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f14c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ISO_14067 | ISO_14067::p0003::chunk0000 | page=3]\n",
      "EXTRACTO DEL DOCUMENTO UNE-EN ISO 14067 6.4.2 Recopilación de datos ................................................................................................... 34 6.4.3 Validación de datos ........................................................................................................ 35 6.4.4 Relación de los datos con los procesos unitarios y la unidad funcional o declarada .................................................................................................... 35 6.4.5 Ajuste de los límites del sistema ............................................................................... 35 6.4.6 Asignación ......................................................................................................................... 36 6.4.7 Seguimiento del desempeño de la HCP ................................................................... 38 6.4.8 Evaluación del efe\n",
      "\n",
      "[ISO_14067 | ISO_14067::p0001::chunk0000 | page=1]\n",
      "UNE-EN ISO 14067 Gases de efecto invernadero Huella de carbono de productos Requisitos y directrices para cuantificación (ISO 14067:2018) Greenhouse gases. Carbon footprint of products. Requirements and guidelines for quantification (ISO 14067:2018). Gaz à effet de serre. Empreinte carbone des produits. Exigences et lignes directrices pour la quantification (ISO 14067:2018). Esta norma es la versión oficial, en español, de la Norma Europea EN ISO 14067:2018, que a su vez adopta la Norma Internacional ISO 14067:2018. Esta norma anula y sustituye a la Especificación Técnica UNE-CEN ISO/TS 14067:2015. Las observaciones a este documento han de dirigirse a: Asociación Española de Normalización Génova, 6 28004 MADRID-España Tel.: 915 294 900 info@une.org www.une.org Depósito legal: M 30327:2019 © UNE 2019 Prohibida la reproducción sin el consentimiento de UNE. Todos los derechos de propiedad i\n",
      "\n",
      "[ISO_14067 | ISO_14067::p0004::chunk0000 | page=4]\n",
      "EXTRACTO DEL DOCUMENTO UNE-EN ISO 14067 2 Normas para consulta Los siguientes documentos se referencian en el texto de tal forma que parte o la totalidad de su contenido constituyen requisitos de este documento. Para las referencias con fecha, sólo aplica la edición citada. Para las referencias sin fecha se aplica la última edición del documento de referencia (incluyendo cualquier modificación). ISO/TS 14027:2017, Etiquetas y declaraciones ambientales. Desarrollo de reglas de categoría de producto. ISO 14044:2006, Gestión ambiental. Análisis del ciclo de vida. Requisitos y directrices. ISO/TS 14071, Environmental management. Life cycle assessment. Critical review processes and reviewer competencies: Additional requirements and guidelines to ISO 14044:2006.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def retrieve_context(query: str, top_k: int = 4, filter_doc_ids: List[str] | None = None) -> str:\n",
    "    \"\"\"Devuelve un string con los top-k chunks (útil para debug rápido).\"\"\"\n",
    "    q_emb = embed_model.encode([query], normalize_embeddings=True)[0]\n",
    "    scores = np.dot(chunk_embs, q_emb)\n",
    "\n",
    "    # Filtrar por doc_ids si los damos (ej: ISO_14067)\n",
    "    if filter_doc_ids:\n",
    "        mask = np.array([c[\"doc_id\"] in set(filter_doc_ids) for c in all_chunks], dtype=bool)\n",
    "        scores = np.where(mask, scores, -1e9)\n",
    "\n",
    "    top_idx = np.argsort(scores)[-top_k:][::-1]\n",
    "    selected = [all_chunks[i] for i in top_idx]\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{s['doc_id']} | {s['chunk_id']} | page={s.get('page', -1)}]\\n{s['text']}\"\n",
    "        for s in selected\n",
    "    )\n",
    "    return context\n",
    "\n",
    "\n",
    "def retrieve_context_items(\n",
    "    query: str,\n",
    "    top_k: int = 4,\n",
    "    filter_doc_ids: List[str] | None = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Devuelve items con metadata (doc_id/page/chunk_id/score/text) para que el LLM pueda citar evidencia.\"\"\"\n",
    "    q_emb = embed_model.encode([query], normalize_embeddings=True)[0]\n",
    "    scores = np.dot(chunk_embs, q_emb)\n",
    "\n",
    "    if filter_doc_ids:\n",
    "        allowed = set(filter_doc_ids)\n",
    "        mask = np.array([c[\"doc_id\"] in allowed for c in all_chunks], dtype=bool)\n",
    "        scores = np.where(mask, scores, -1e9)\n",
    "\n",
    "    top_idx = np.argsort(scores)[-top_k:][::-1]\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    for i in top_idx:\n",
    "        c = all_chunks[i]\n",
    "        items.append({\n",
    "            \"doc_id\": c[\"doc_id\"],\n",
    "            \"page\": int(c.get(\"page\", -1)),\n",
    "            \"chunk_id\": c[\"chunk_id\"],\n",
    "            \"score\": float(scores[i]),\n",
    "            \"text\": c[\"text\"],\n",
    "        })\n",
    "    return items\n",
    "\n",
    "\n",
    "def format_items_for_prompt(items: List[Dict[str, Any]], max_chars: int = 2500) -> str:\n",
    "    \"\"\"Convierte items a un bloque compacto con citas. Recorta para no explotar tokens.\"\"\"\n",
    "    lines = []\n",
    "    for it in items:\n",
    "        snippet = (it.get(\"text\") or \"\").strip().replace(\"\\n\", \" \")\n",
    "        snippet = snippet[:900]\n",
    "        lines.append(\n",
    "            f\"- [doc={it['doc_id']}|page={it['page']}|chunk={it['chunk_id']}|score={it['score']:.3f}] {snippet}\"\n",
    "        )\n",
    "    s = \"\\n\".join(lines)\n",
    "    if len(s) > max_chars:\n",
    "        s = s[:max_chars] + \"\\n[...TRUNCATED ITEMS...]\"\n",
    "    return s\n",
    "\n",
    "\n",
    "# prueba rápida (debug)\n",
    "print(retrieve_context(\"ISO 14067 unidad funcional límites del sistema reporte verificación\", top_k=3, filter_doc_ids=[\"ISO_14067\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43527202",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Eres un auditor técnico de cumplimiento normativo y huella de carbono para obras de construcción en México.\n",
    "Tu tarea: evaluar si la obra cumple o qué información falta para evaluar el cumplimiento según normas objetivo.\n",
    "\n",
    "IMPORTANTE:\n",
    "- Usa ÚNICAMENTE la evidencia del JSON de entrada y el EVIDENCE_PACK (proveniente del RAG).\n",
    "- NO inventes datos. Si falta algo, marca 'needs_info' y lista faltantes concretos y preguntas.\n",
    "- Si el EVIDENCE_PACK contiene citas [doc=...|page=...|chunk=...], debes usarlas en 'evidencia_usada'.\n",
    "- Devuelve SIEMPRE un JSON válido (sin texto extra) con el esquema solicitado.\"\"\"\n",
    "\n",
    "OUTPUT_SCHEMA_HINT = \"\"\"\n",
    "Debes devolver EXACTAMENTE este formato JSON:\n",
    "\n",
    "{\n",
    "  \"obra_id\": \"...\",\n",
    "  \"evaluacion_fecha\": \"YYYY-MM-DD\",\n",
    "  \"resultado_global\": {\n",
    "    \"estatus\": \"pass|fail|needs_info\",\n",
    "    \"riesgo\": \"bajo|medio|alto\",\n",
    "    \"razones\": [\"...\"]\n",
    "  },\n",
    "  \"por_norma\": [\n",
    "    {\n",
    "      \"norma\": \"...\",\n",
    "      \"estatus\": \"pass|fail|needs_info\",\n",
    "      \"hallazgos\": [\"...\"],\n",
    "      \"evidencia_usada\": [\n",
    "        {\n",
    "          \"tipo\": \"json|rag\",\n",
    "          \"ref\": \"materiales[0].factor_emision_kgco2e_por_unidad | doc=ISO_14067|page=12|chunk=...\",\n",
    "          \"extracto\": \"...\"\n",
    "        }\n",
    "      ],\n",
    "      \"faltantes\": [\"...\"],\n",
    "      \"acciones_recomendadas\": [\"...\"]\n",
    "    }\n",
    "  ],\n",
    "  \"preguntas_para_completar\": [\"...\"]\n",
    "}\n",
    "\n",
    "Reglas:\n",
    "- 'pass' solo si hay evidencia suficiente.\n",
    "- 'fail' si hay evidencia de incumplimiento.\n",
    "- 'needs_info' si faltan datos para dictaminar.\n",
    "- Si existe evidencia RAG (citas con doc/page/chunk), 'evidencia_usada' NO puede quedar vacía.\n",
    "- 'evidencia_usada' puede referir rutas del JSON (ej: \"materiales[0].co2e_kg\") o citas del RAG.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd2017c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normas objetivo: ['RENE', 'ISO_14064-1', 'ISO_14067', 'GHG_Protocol_Scope3', 'NACDMX-007-RNAT-2019', 'NADF-018-AMBT-2009', 'Ley_Residuos_Solidos_CDMX', 'Reglamento_Construcciones_CDMX', 'NOM-001-SEDE-2012']\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 42 column 6 (char 1800)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mllm_json\u001b[0;34m(system_prompt, user_obj, max_tokens)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_try_parse_json_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# último intento: recortar a bloque JSON\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36m_try_parse_json_local\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     31\u001b[0m     s \u001b[38;5;241m=\u001b[39m s[start:end\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 42 column 6 (char 1800)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 189\u001b[0m\n\u001b[1;32m    186\u001b[0m normas_objetivo \u001b[38;5;241m=\u001b[39m extract_normas(payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormativa_objetivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormas objetivo:\u001b[39m\u001b[38;5;124m\"\u001b[39m, normas_objetivo)\n\u001b[0;32m--> 189\u001b[0m evidence_pack \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_evidence_pack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormas_objetivo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_items_per_norma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocs missing:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evidence_pack\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs_missing\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPor norma:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(evidence_pack\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpor_norma\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])))\n",
      "Cell \u001b[0;32mIn[11], line 132\u001b[0m, in \u001b[0;36mbuild_evidence_pack\u001b[0;34m(normas, payload, max_items_per_norma)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_pages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rag_pages, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    130\u001b[0m     doc_ids_available \u001b[38;5;241m=\u001b[39m {p\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rag_pages \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[0;32m--> 132\u001b[0m plans \u001b[38;5;241m=\u001b[39m \u001b[43mplan_queries_for_normas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m por_norma \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m docs_missing \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[11], line 78\u001b[0m, in \u001b[0;36mplan_queries_for_normas\u001b[0;34m(normas, payload)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplan_queries_for_normas\u001b[39m(normas: List[\u001b[38;5;28mstr\u001b[39m], payload: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     72\u001b[0m     payload_small \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobra_tipo\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobra_tipo\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mubicacion\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mubicacion\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmateriales\u001b[39m\u001b[38;5;124m\"\u001b[39m: [m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaterial_nombre\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmateriales\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])],\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormas\u001b[39m\u001b[38;5;124m\"\u001b[39m: normas,\n\u001b[1;32m     77\u001b[0m     }\n\u001b[0;32m---> 78\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mllm_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPLANNER_SYSTEM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m700\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     plans \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplans\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m, in \u001b[0;36mllm_json\u001b[0;34m(system_prompt, user_obj, max_tokens)\u001b[0m\n\u001b[1;32m     49\u001b[0m end \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m end \u001b[38;5;241m>\u001b[39m start:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 42 column 6 (char 1800)"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_normas(normativa_objetivo: Dict[str, Any]) -> List[str]:\n",
    "    normas = []\n",
    "    for _, lst in (normativa_objetivo or {}).items():\n",
    "        if isinstance(lst, list):\n",
    "            normas.extend(lst)\n",
    "    # unique preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for n in normas:\n",
    "        if n not in seen:\n",
    "            out.append(n)\n",
    "            seen.add(n)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ==========\n",
    "# LLM chiquito como \"planner\" y \"extractor\" (agentic RAG)\n",
    "# ==========\n",
    "\n",
    "def _try_parse_json_local(s: str) -> Dict[str, Any]:\n",
    "    s = (s or \"\").strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```(json)?\", \"\", s).strip()\n",
    "        s = re.sub(r\"```$\", \"\", s).strip()\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start >= 0 and end >= 0 and end > start:\n",
    "        s = s[start:end+1]\n",
    "    return json.loads(s)\n",
    "\n",
    "def llm_json(system_prompt: str, user_obj: Dict[str, Any], max_tokens: int = 700) -> Dict[str, Any]:\n",
    "    resp = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(user_obj, ensure_ascii=False)},\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    raw = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    try:\n",
    "        return _try_parse_json_local(raw)\n",
    "    except Exception:\n",
    "        # último intento: recortar a bloque JSON\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            return json.loads(raw[start:end+1])\n",
    "        raise\n",
    "\n",
    "\n",
    "PLANNER_SYSTEM = \"\"\"Eres un asistente que SOLO genera queries de búsqueda para un RAG de normas.\n",
    "Devuelve SOLO JSON válido.\n",
    "\n",
    "Reglas:\n",
    "- Para cada norma: 2 a 4 queries MUY específicas (no genéricas).\n",
    "- Prioriza texto normativo: \"debe\", \"deberá\", \"obligatorio\", \"requisito\", \"registro\", \"bitácora\", \"anexo\", \"sanción\".\n",
    "- Incluye expected_evidence (palabras clave esperadas).\n",
    "- top_k por norma: 2 o 3 (máximo).\n",
    "Esquema exacto:\n",
    "{\n",
    "  \"plans\": [\n",
    "    {\"norma\": \"...\", \"queries\": [\"...\"], \"expected_evidence\": [\"...\"], \"top_k\": 2}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def plan_queries_for_normas(normas: List[str], payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    payload_small = {\n",
    "        \"obra_tipo\": payload.get(\"obra_tipo\"),\n",
    "        \"ubicacion\": payload.get(\"ubicacion\"),\n",
    "        \"materiales\": [m.get(\"material_nombre\") for m in payload.get(\"materiales\", [])],\n",
    "        \"normas\": normas,\n",
    "    }\n",
    "    out = llm_json(PLANNER_SYSTEM, payload_small, max_tokens=700)\n",
    "    plans = {}\n",
    "    for p in out.get(\"plans\", []):\n",
    "        n = p.get(\"norma\")\n",
    "        if n:\n",
    "            plans[n] = p\n",
    "    return plans\n",
    "\n",
    "\n",
    "EXTRACTOR_SYSTEM = \"\"\"Eres un auditor técnico. Te paso fragmentos (chunks) con metadatos (doc_id/page/chunk_id).\n",
    "Devuelve SOLO JSON válido.\n",
    "\n",
    "Objetivo:\n",
    "- Extraer requisitos/obligaciones aplicables a la obra\n",
    "- Identificar evidencia usada (copiando doc_id/page/chunk_id)\n",
    "- Indicar faltantes y preguntas concretas\n",
    "\n",
    "Reglas:\n",
    "- Si hay chunks, evidencia_usada NO puede estar vacía.\n",
    "- En evidencia_usada: incluye extracto <= 200 caracteres.\n",
    "Esquema:\n",
    "{\n",
    "  \"norma\": \"...\",\n",
    "  \"hallazgos\": [\"...\"],\n",
    "  \"evidencia_usada\": [{\"doc_id\":\"...\",\"page\":12,\"chunk_id\":\"...\",\"extracto\":\"...\"}],\n",
    "  \"faltantes\": [\"...\"],\n",
    "  \"preguntas\": [\"...\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def extract_from_items(norma: str, items: List[Dict[str, Any]], payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    prompt_payload = {\n",
    "        \"norma\": norma,\n",
    "        \"obra\": {\n",
    "            \"obra_tipo\": payload.get(\"obra_tipo\"),\n",
    "            \"ubicacion\": payload.get(\"ubicacion\"),\n",
    "            \"materiales\": [m.get(\"material_nombre\") for m in payload.get(\"materiales\", [])],\n",
    "            \"co2e_total_kg\": payload.get(\"resumen\", {}).get(\"co2e_total_kg\"),\n",
    "        },\n",
    "        \"chunks\": items,\n",
    "    }\n",
    "    return llm_json(EXTRACTOR_SYSTEM, prompt_payload, max_tokens=900)\n",
    "\n",
    "\n",
    "def build_evidence_pack(normas: List[str], payload: Dict[str, Any], max_items_per_norma: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"Planner -> Retriever -> Extractor. Devuelve un paquete compacto para el LLM final.\"\"\"\n",
    "    if not normas:\n",
    "        return {\"por_norma\": [], \"docs_missing\": []}\n",
    "\n",
    "    # doc ids disponibles\n",
    "    doc_ids_available = set()\n",
    "    if \"rag_pages\" in globals() and isinstance(rag_pages, list):\n",
    "        doc_ids_available = {p.get(\"doc_id\") for p in rag_pages if isinstance(p, dict) and p.get(\"doc_id\")}\n",
    "\n",
    "    plans = plan_queries_for_normas(normas, payload)\n",
    "\n",
    "    por_norma = []\n",
    "    docs_missing = []\n",
    "\n",
    "    for norma in normas:\n",
    "        plan = plans.get(norma, {\n",
    "            \"norma\": norma,\n",
    "            \"queries\": [f\"{norma} requisitos obligatorios evidencia documental debe deberá anexo registro bitácora sanción\"],\n",
    "            \"expected_evidence\": [],\n",
    "            \"top_k\": 2\n",
    "        })\n",
    "\n",
    "        filter_doc = [norma] if norma in doc_ids_available else None\n",
    "        if filter_doc is None:\n",
    "            docs_missing.append(norma)\n",
    "\n",
    "        pool = []\n",
    "        seen = set()\n",
    "\n",
    "        for q in (plan.get(\"queries\") or [])[:4]:\n",
    "            items = retrieve_context_items(q, top_k=int(plan.get(\"top_k\", 2)), filter_doc_ids=filter_doc)\n",
    "            for it in items:\n",
    "                if it[\"chunk_id\"] not in seen:\n",
    "                    seen.add(it[\"chunk_id\"])\n",
    "                    # recorta texto por seguridad\n",
    "                    it2 = dict(it)\n",
    "                    it2[\"text\"] = (it2.get(\"text\") or \"\")[:1200]\n",
    "                    pool.append(it2)\n",
    "\n",
    "        pool = sorted(pool, key=lambda x: x.get(\"score\", 0.0), reverse=True)[:max_items_per_norma]\n",
    "\n",
    "        if pool:\n",
    "            extracted = extract_from_items(norma, pool, payload)\n",
    "        else:\n",
    "            extracted = {\n",
    "                \"norma\": norma,\n",
    "                \"hallazgos\": [],\n",
    "                \"evidencia_usada\": [],\n",
    "                \"faltantes\": [\n",
    "                    \"No se encontró evidencia en el RAG para esta norma (PDF no disponible, páginas no seleccionadas, o queries insuficientes).\"\n",
    "                ],\n",
    "                \"preguntas\": [f\"¿Puedes proporcionar el PDF o indicar páginas/secciones clave de {norma}?\"]\n",
    "            }\n",
    "\n",
    "        por_norma.append(extracted)\n",
    "\n",
    "    return {\"por_norma\": por_norma, \"docs_missing\": list(dict.fromkeys(docs_missing))}\n",
    "\n",
    "\n",
    "# ==========\n",
    "# Ejecución: generar EVIDENCE_PACK y luego el JSON final\n",
    "# ==========\n",
    "\n",
    "normas_objetivo = extract_normas(payload.get(\"normativa_objetivo\", {}))\n",
    "print(\"Normas objetivo:\", normas_objetivo)\n",
    "\n",
    "evidence_pack = build_evidence_pack(normas_objetivo, payload, max_items_per_norma=4)\n",
    "print(\"Docs missing:\", evidence_pack.get(\"docs_missing\"))\n",
    "print(\"Por norma:\", len(evidence_pack.get(\"por_norma\", [])))\n",
    "\n",
    "today_str = date.today().isoformat()\n",
    "\n",
    "# Payload reducido para ahorrar tokens\n",
    "payload_llm = {\n",
    "    \"obra_id\": payload.get(\"obra_id\"),\n",
    "    \"obra_tipo\": payload.get(\"obra_tipo\"),\n",
    "    \"obra_descripcion\": payload.get(\"obra_descripcion\"),\n",
    "    \"ubicacion\": payload.get(\"ubicacion\"),\n",
    "    \"normativa_objetivo\": payload.get(\"normativa_objetivo\"),\n",
    "    \"materiales\": [\n",
    "        {\n",
    "            \"material_nombre\": m.get(\"material_nombre\"),\n",
    "            \"categoria\": m.get(\"categoria\"),\n",
    "            \"cantidad\": m.get(\"cantidad\"),\n",
    "            \"unidad\": m.get(\"unidad\"),\n",
    "            \"co2e_kg\": m.get(\"co2e_kg\"),\n",
    "            \"factor_emision_kgco2e_por_unidad\": m.get(\"factor_emision_kgco2e_por_unidad\"),\n",
    "            \"proveedor\": m.get(\"proveedor\", {}),\n",
    "        }\n",
    "        for m in (payload.get(\"materiales\") or [])\n",
    "    ],\n",
    "    \"resumen\": {\n",
    "        \"co2e_total_kg\": (payload.get(\"resumen\") or {}).get(\"co2e_total_kg\"),\n",
    "        \"notas\": (payload.get(\"resumen\") or {}).get(\"notas\", []),\n",
    "    },\n",
    "}\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "INPUT_JSON_OBRA (reducido):\n",
    "{json.dumps(payload_llm, ensure_ascii=False, indent=2)}\n",
    "\n",
    "EVIDENCE_PACK (RAG resumido con citas doc/page/chunk):\n",
    "{json.dumps(evidence_pack, ensure_ascii=False, indent=2)}\n",
    "\n",
    "{OUTPUT_SCHEMA_HINT}\n",
    "\n",
    "Hoy es: {today_str}\n",
    "\"\"\"\n",
    "\n",
    "resp = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=1800,\n",
    ")\n",
    "\n",
    "raw = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "raw[:600]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d72ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obra_id': 'OBRA-001',\n",
       " 'evaluacion_fecha': '2026-02-01',\n",
       " 'resultado_global': {'estatus': 'needs_info',\n",
       "  'riesgo': 'medio',\n",
       "  'razones': ['Falta información sobre la implementación de RCD y control de polvo según NACDMX-007-RNAT-2019 y NADF-018-AMBT-2009.',\n",
       "   'Falta información sobre la emisión de CO2 de los materiales según normas ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3.',\n",
       "   'Falta información sobre la emisión de CO2e de los materiales según normas ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3.']},\n",
       " 'por_norma': [{'norma': 'RENE',\n",
       "   'estatus': 'needs_info',\n",
       "   'evidencia_usada': [],\n",
       "   'faltantes': ['Implementación de RCD y control de polvo según NACDMX-007-RNAT-2019 y NADF-018-AMBT-2009.'],\n",
       "   'acciones_recomendadas': ['Revisar la implementación de RCD y control de polvo según NACDMX-007-RNAT-2019 y NADF-018-AMBT-2009.']},\n",
       "  {'norma': 'ISO_14064-1',\n",
       "   'estatus': 'needs_info',\n",
       "   'evidencia_usada': [],\n",
       "   'faltantes': ['Emisión de CO2 de los materiales según normas ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3.'],\n",
       "   'acciones_recomendadas': ['Calcular la emisión de CO2 de los materiales según normas ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3.']},\n",
       "  {'norma': 'ISO_14067',\n",
       "   'estatus': 'needs_info',\n",
       "   'evidencia_usada': [],\n",
       "   'faltantes': ['Emisión de CO2e de los materiales según normas ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3.'],\n",
       "   'acciones_recomendadas': ['Calcular la emisión de CO2e de los materiales según normas ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3.']}],\n",
       " 'preguntas_para_completar': ['¿Se han realizado los dictámenes técnicos de estabilidad y seguridad estructural según el Reglamento de Construcciones CDMX?',\n",
       "  '¿Se han generado los Constancias de seguridad estructural cumpliendo con las NTC-RSEE?',\n",
       "  '¿Se han realizado acciones de la Administración para la atención de emergencias mayores, como la revisión de seguridad estructural y la rehabilitación sísmica de edificios?',\n",
       "  '¿Se han generado el checklist de RCD y control de polvo según NACDMX-007-RNAT-2019 y NADF-018-AMBT-2009?',\n",
       "  '¿Se han realizado las inspecciones de las estructuras colindantes durante la obra?',\n",
       "  '¿Se han realizado las verificaciones para pequeños y grandes generadores según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han realizado las verificaciones para centros de acopio y/o centros de transferencia según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han realizado las verificaciones para prestadores de servicio de recolección y transporte de los residuos de la construcción y demolición según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han realizado las verificaciones para plantas de reciclaje de residuos de la construcción y demolición según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han generado los informes de datos sobre los GEI según ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3?',\n",
       "  '¿Se han realizado las inspecciones de las estructuras colindantes durante la obra?',\n",
       "  '¿Se han realizado las verificaciones para centros de acopio y/o centros de transferencia según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han realizado las verificaciones para prestadores de servicio de recolección y transporte de los residuos de la construcción y demolición según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han realizado las verificaciones para plantas de reciclaje de residuos de la construcción y demolición según NACDMX-007-RNAT-2019?',\n",
       "  '¿Se han generado los informes de datos sobre los GEI según ISO_14064-1, ISO_14067 y GHG_Protocol_Scope3?']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_parse_json(s: str) -> Dict[str, Any]:\n",
    "    # intenta detectar el primer bloque JSON\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```(json)?\", \"\", s).strip()\n",
    "        s = re.sub(r\"```$\", \"\", s).strip()\n",
    "    # extraer desde la primera { hasta la última }\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start >= 0 and end >= 0:\n",
    "        s = s[start:end+1]\n",
    "    return json.loads(s)\n",
    "\n",
    "def fix_json_with_llm(bad_text: str) -> str:\n",
    "    fix_prompt = f\"\"\"\n",
    "Arregla la salida para que sea SOLO un JSON válido y que respete exactamente el esquema.\n",
    "NO agregues texto extra. SOLO JSON.\n",
    "\n",
    "Salida a corregir:\n",
    "{bad_text}\n",
    "\"\"\"\n",
    "    r = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un corrector estricto de JSON. Devuelve SOLO JSON válido.\"},\n",
    "            {\"role\": \"user\", \"content\": fix_prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=1200,\n",
    "    )\n",
    "    return r[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "try:\n",
    "    out_json = try_parse_json(raw)\n",
    "except Exception as e:\n",
    "    print(\"JSON parse failed, repairing...\", e)\n",
    "    repaired = fix_json_with_llm(raw)\n",
    "    out_json = try_parse_json(repaired)\n",
    "\n",
    "out_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/harielpadillasanchez/Documentos/hackathon/Tec_environmental_risk/Data/output_LLM.json\n"
     ]
    }
   ],
   "source": [
    "# Asegurar obra_id y fecha si el modelo no los pone bien\n",
    "out_json[\"obra_id\"] = payload.get(\"obra_id\", out_json.get(\"obra_id\"))\n",
    "out_json[\"evaluacion_fecha\"] = out_json.get(\"evaluacion_fecha\") or today_str\n",
    "\n",
    "OUTPUT_JSON_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", OUTPUT_JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatus global: needs_info\n",
      "Normas evaluadas: 3\n",
      "Preguntas: 15\n"
     ]
    }
   ],
   "source": [
    "assert \"resultado_global\" in out_json\n",
    "assert \"por_norma\" in out_json\n",
    "assert isinstance(out_json[\"por_norma\"], list)\n",
    "\n",
    "print(\"Estatus global:\", out_json[\"resultado_global\"][\"estatus\"])\n",
    "print(\"Normas evaluadas:\", len(out_json[\"por_norma\"]))\n",
    "print(\"Preguntas:\", len(out_json.get(\"preguntas_para_completar\", [])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
